{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas                        as pd\n",
    "import numpy                         as np\n",
    "import seaborn                       as sns\n",
    "import matplotlib.pyplot             as plt\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.metrics                 import confusion_matrix\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.stem                       import WordNetLemmatizer\n",
    "from nltk.tokenize                   import RegexpTokenizer \n",
    "from IPython.core.display            import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "-------\n",
    "\n",
    "1. [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Overview](#Overview)\n",
    "    - [Visuals](#Visuals)\n",
    "    \n",
    "    \n",
    "2. [Lemmatizing](#Lemmatizing)\n",
    "\n",
    "\n",
    "3. [Establishing The Baseline](#Establishing-The-Baseline)\n",
    "\n",
    "\n",
    "4. [Modeling](#Modeling)\n",
    "    - [Setting The X and y variables](#Setting-The-X-and-y-variables)\n",
    "    - [Running A Train-Test Split](#Running-A-Train-Test-Split)\n",
    "    - [Logistic Regression](#Logistic-Regression)\n",
    "    - [Random Forest Classification](#Random-Forest-Classification)\n",
    "    - [XGboost Classifier](#XGBoost-Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.read_csv(\"../Data/model_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data's head\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "model_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types\n",
    "\n",
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_length_dist(list):\n",
    "    plt.figure(figsize = (18,6))\n",
    "    sns.distplot(list, kde = False, color = \"black\",\n",
    "                 bins = 60)\n",
    "    plt.title(f\"Distribution Of Text Lengths\", size = 18)\n",
    "    plt.xlabel(\"Length\", size = 16)\n",
    "    plt.ylabel(\"Frequency\", size = 16)\n",
    "    plt.xticks(np.arange(0,23500,1500), size = 14)\n",
    "    plt.yticks(size = 14)\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_frequent_authors(df, col):\n",
    "    \n",
    "    plt.figure(figsize = (20,6))\n",
    "    sns.barplot(x = df.index,\n",
    "                y = col,\n",
    "                data = df)\n",
    "    plt.title(\"Most Common Posters\", size = 18)\n",
    "    plt.xlabel(\"Reddit User\", size = 16)\n",
    "    plt.ylabel(\"Number Of Posts\", size = 16)\n",
    "    plt.xticks(size = 13)\n",
    "    plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a list of text lengths\n",
    "\n",
    "length_list = [len(text) for text in model_data[\"text\"]]\n",
    "\n",
    "plot_text_length_dist(length_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the posts are relatively short (<2000 words), but there are a few that are extremely long (>20,000 words.)  We expected that most posts would be less than a few thousand words, which is true for the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Most Frequent Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_count = pd.DataFrame(model_data[\"author\"].value_counts().head(10))\n",
    "\n",
    "plot_most_frequent_authors(df  = author_count, \n",
    "                           col = \"author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not really know what to expect when we plotted this graph, because it is generally the case that a few users post most frequently and most barely post at all.  We would have like to look at the number of comments by each user in both subreddits as a measure of activity, but that is beyond the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subreddit Of Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels = [\"r/Cooking\", \"r/AskCulinary\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(model_data[\"source\"])\n",
    "plt.title(\"Post Origin\", size = 18)\n",
    "plt.xlabel(\"Source\", size = 16)\n",
    "plt.ylabel(\"Number Of Posts\", size = 16)\n",
    "plt.xticks(np.arange(0,2,1), \n",
    "           labels = tick_labels, \n",
    "           size = 14)\n",
    "plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were a little surprised that there are more r/AskCulinary posts because we had roughly equal numbers of pulls from each subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Most Common Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start modeling, we need to know what the most frequent words are in each subreddit are because it might be harder for our model to predict with those words in the dataframe.\n",
    "\n",
    "We will subset the data frame into posts from r/Cooking and r/AskCulinary and use count vectorizer to determine the most frequent words.  We will also remove stop words from the outset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_frequent_words(dataframes, titles):\n",
    "    count = 0\n",
    "    fig   = plt.figure(figsize = (24,20))\n",
    "    for d, dataframe in enumerate(dataframes):\n",
    "        count += 1\n",
    "        ax    = fig.add_subplot(2, 2, count)\n",
    "        sns.barplot(x       = 0,\n",
    "                    y       = dataframe.index,\n",
    "                    data    = dataframe,\n",
    "                    palette = \"deep\")\n",
    "        plt.title(f\"Most Common Words From {titles[d]}\", size = 20)\n",
    "        plt.xlabel(\"Number Of Occurences\", size = 18)\n",
    "        plt.ylabel(\"Word\", size = 18)\n",
    "        plt.xticks(size = 16)\n",
    "        plt.yticks(size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the count vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Masking the vectorizer with English stop words\n",
    "\n",
    "cvec_cooking     = CountVectorizer(stop_words = \"english\")\n",
    "cvec_askculinary = CountVectorizer(stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the dataframe\n",
    "\n",
    "cooking     = model_data[model_data[\"target\"] == 1]\n",
    "askculinary = model_data[model_data[\"target\"] == 0]\n",
    "\n",
    "# Fit-transforming the vectorizer\n",
    "\n",
    "vec_cooking     = cvec_cooking.fit_transform(cooking[\"text\"])\n",
    "vec_askculinary = cvec_askculinary.fit_transform(askculinary[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vectorized dfs to a new dataframe\n",
    "\n",
    "cooking_vectorized = pd.DataFrame(vec_cooking.toarray(), \n",
    "                                  columns = cvec_cooking.get_feature_names())\n",
    "\n",
    "askculinary_vectorized = pd.DataFrame(vec_askculinary.toarray(), \n",
    "                                      columns = cvec_askculinary.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the 15 most frequent words from each\n",
    "\n",
    "vectorized_cooking     = pd.DataFrame(cooking_vectorized.sum().sort_values(ascending = False).head(15))\n",
    "vectorized_askculinary = pd.DataFrame(askculinary_vectorized.sum().sort_values(ascending = False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the most common words\n",
    "\n",
    "plot_most_frequent_words(dataframes = [vectorized_cooking, vectorized_askculinary],\n",
    "                         titles     = [\"r/Cooking\", \"r/AskCulinary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of words that occur in both subreddits.  We decided that because of that, we should create a list of customized stop words.  Furthermore, we noticed that we have to lemmatize or stem the text columns because of there are multiple forms of words in the most frequent words such as 'make' & 'making' or 'recipe' and recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the default stopwords\n",
    "\n",
    "nltk.download(\"stopwords\");\n",
    "\n",
    "# Adding our stopwords to the English set\n",
    "\n",
    "new_stopwords = [\"like\", \"just\", \"make\", \"cook\",\n",
    "                 \"use\", \"chicken\", \"recipe\", \"sauce\"]\n",
    "\n",
    "stopwords     = stopwords.words('english')\n",
    "\n",
    "stopwords.extend(new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We felt that lemmatizing is a better option than stemming because the lemma form of a word is more likely to result in an actual word of English than trying to find a word's stem: there are so many irregularities in English that it is not always easy to find the stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the lemmatizier and tokenizer\n",
    "# The tokenizer will only keep text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer  = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Setting up the lemmatizer\n",
    "\n",
    "lemmatized_posts = []\n",
    "\n",
    "for post in model_data[\"text\"]:\n",
    "    tokens = tokenizer.tokenize(post)\n",
    "    post  = [lemmatizer.lemmatize(post) for post in tokens]\n",
    "    lemmatized_posts.append(\" \".join(post))\n",
    "    \n",
    "# Appending the lemmatized posts to the dataframe\n",
    "\n",
    "model_data[\"lemmatized_text\"] = lemmatized_posts\n",
    "\n",
    "# Checking the head of the dataframe\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While checking the results from the cell above, we noticed that in `lemmatized_text` there are some URLs which need to be removed.  We used a regular expression to remove all URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[\"lemmatized_text\"] = model_data[\"lemmatized_text\"].str.replace(\"http\\S+\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline in classification gives us an idea of how exactly the model is performing.  The baseline is simply the percentage of occurrences of our target in the data as a whole.  In this case it will be what percentage of posts are from r/Cooking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model has an accuracy of >41.44% we know that it is better than simply guessing the class of a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(model_data[\"target\"].value_counts(normalize = True)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is in the format we want, we can begin the process of modeling.\n",
    "\n",
    "There are a few steps we have to do before we start running models: we have to define the X and y variables and run a train-test split on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting The X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data[\"lemmatized_text\"]\n",
    "y = model_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running A Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A train-test split is important because it allows us to reserve a portion of our data for test so that the model does not see all data before predicting.  In this case we want to preserve the class split, so we will stratify the data to match the distribution of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stratify argument preserves the distribution of classes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the three models we will use will be gridsearched so that we can experiment with different combinations of hyperparameters (parameters we have to define).  Additionally, each model will be fit with a count vectorizer and a TFIDF (Term Frequency-Inverse Document Frequency) vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is very similar to the linear regression, but it uses a logit function to bend the line so that it can predict either 0 or 1.\n",
    "\n",
    "\n",
    "The gridsearch will be searching hyperparameters for the count vectorizer and the TFIDF, not the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "\n",
    "cvec_lr_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"log_reg\", LogisticRegression())])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\": [125], \n",
    "                    \"cvec__ngram_range\": [(1,2)], \n",
    "                    \"cvec__stop_words\": [None]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_lr_gs = GridSearchCV(cvec_lr_pipe, \n",
    "                          param_grid = cvec_pipe_params, \n",
    "                          cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "cvec_lr_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting TFIDF pipe parameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\": [100, 250, 500], \n",
    "                    \"tvec__ngram_range\" : [(1,1),(1,2)], \n",
    "                    \"tvec__stop_words\"  : [None]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_lr_gs = GridSearchCV(tvec_lr_pipe, \n",
    "                          param_grid = tvec_pipe_params, \n",
    "                          cv         = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up my pipeline\n",
    "\n",
    "tfidf_lr_pipe = Pipeline([(\"tfidf\", TfidfVectorizer()), \n",
    "                          (\"log_reg\", LogisticRegression())])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "tfidf_pipe_params = {\"tfidf__max_features\": [2375], \n",
    "                     \"tfidf__ngram_range\": [(1,1)], \n",
    "                     \"tfidf__stop_words\": [None]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "tfidf_lr_gs = GridSearchCV(tfidf_lr_pipe, \n",
    "                           param_grid = tfidf_pipe_params, \n",
    "                           cv = 5)\n",
    "\n",
    "# Fitting the model to my training data\n",
    "\n",
    "tfidf_lr_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
