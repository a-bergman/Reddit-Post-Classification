{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas                        as pd\n",
    "import numpy                         as np\n",
    "import seaborn                       as sns\n",
    "import matplotlib.pyplot             as plt\n",
    "import scikitplot                   as skplt\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.metrics                 import confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics                 import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics                 import balanced_accuracy_score, roc_curve\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.svm                     import SVC\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.stem                       import WordNetLemmatizer\n",
    "from nltk.tokenize                   import RegexpTokenizer \n",
    "from xgboost                         import XGBClassifier\n",
    "from IPython.display                 import display_html\n",
    "from IPython.core.display            import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "\n",
    "\n",
    "- [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Overview](#Overview)\n",
    "    - [Visuals](#Visuals)\n",
    "    \n",
    "    \n",
    "- [Lemmatizing](#Lemmatizing)\n",
    "\n",
    "\n",
    "- [Establishing The Baseline](#Establishing-The-Baseline)\n",
    "\n",
    "\n",
    "- [Modeling](#Modeling)\n",
    "    - [Setting The X & y variables](#Setting-The-X-and-y-variables)\n",
    "    - [Running A Train-Test Split](#Running-A-Train-Test-Split)\n",
    "    - [Evaluation Formulae](#Evaluation-Formulae)\n",
    "    - [Logistic Regression](#Logistic-Regression)\n",
    "    - [Random Forest Classifier](#Random-Forest-Classifier)\n",
    "    - [Support Vector Classifier](#Support-Vector-Classifier)\n",
    "    - [XGBoost Classifier](#XGBoost-Classifier)\n",
    "    \n",
    "    \n",
    "- [Evaluation](#Evaluation)\n",
    "    - [Best Model Selection](#Best-Model-Selection)\n",
    "    - [Evaluation Functions](#Evaluation-Functions)\n",
    "    - [Dataframes](#Dataframes)\n",
    "    - [Plots](#Plots)\n",
    "        - [Bar Plot](#Bar-Plot)\n",
    "        - [ROC Curve](#ROC-Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.read_csv(\"../Data/model_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data's head\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "model_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types\n",
    "\n",
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_length_dist(text_list):\n",
    "    \n",
    "    # Setting the figure size\n",
    "    plt.figure(figsize = (18,6))\n",
    "    \n",
    "    # Plotting the histogram\n",
    "    sns.distplot(text_list, kde = False, color = \"black\",\n",
    "                 bins = 60)\n",
    "    \n",
    "    # Setting graph parameters\n",
    "    plt.title(f\"Distribution Of Text Lengths\", size = 18)\n",
    "    plt.xlabel(\"Length\", size = 16)\n",
    "    plt.ylabel(\"Frequency\", size = 16)\n",
    "    plt.xticks(np.arange(0,23500,1500), size = 14)\n",
    "    plt.yticks(size = 14)\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_frequent_authors(df, col):\n",
    "    \n",
    "    # Setting the figure size\n",
    "    plt.figure(figsize = (20,6))\n",
    "    \n",
    "    # Creating the bar chart\n",
    "    sns.barplot(x = df.index,\n",
    "                y = col,\n",
    "                data = df)\n",
    "    \n",
    "    # Setting graph parameters\n",
    "    plt.title(\"Most Common Posters\", size = 18)\n",
    "    plt.xlabel(\"Reddit User\", size = 16)\n",
    "    plt.ylabel(\"Number Of Posts\", size = 16)\n",
    "    plt.xticks(size = 13)\n",
    "    plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a list of text lengths\n",
    "\n",
    "length_list = [len(text) for text in model_data[\"text\"]]\n",
    "\n",
    "plot_text_length_dist(length_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the posts are relatively short (<2000 words), but there are a few that are extremely long (>20,000 words.)  We expected that most posts would be less than a few thousand words, which is true for the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Most Frequent Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_count = pd.DataFrame(model_data[\"author\"].value_counts().head(10))\n",
    "\n",
    "plot_most_frequent_authors(df  = author_count, \n",
    "                           col = \"author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not really know what to expect when we plotted this graph, because it is generally the case that a few users post most frequently and most barely post at all.  We would have like to look at the number of comments by each user in both subreddits as a measure of activity, but that is beyond the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subreddit Of Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels = [\"r/Cooking\", \"r/AskCulinary\"]\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "# Plotting the graph\n",
    "sns.countplot(model_data[\"source\"])\n",
    "\n",
    "# Setting graph parameters\n",
    "plt.title(\"Post Origin\", size = 18)\n",
    "plt.xlabel(\"Source\", size = 16)\n",
    "plt.ylabel(\"Number Of Posts\", size = 16)\n",
    "\n",
    "# Making sure the only two ticks are 0 and 1\n",
    "plt.xticks(np.arange(0,2,1), \n",
    "           labels = tick_labels, \n",
    "           size = 14)\n",
    "plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were a little surprised that there are more r/AskCulinary posts because we had roughly equal numbers of pulls from each subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Most Common Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start modeling, we need to know what the most frequent words are in each subreddit are because it might be harder for our model to predict with those words in the dataframe.\n",
    "\n",
    "We will subset the data frame into posts from r/Cooking and r/AskCulinary and use count vectorizer to determine the most frequent words.  We will also remove stop words from the outset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_frequent_words(dataframes, titles):\n",
    "    \n",
    "    # The count inidcates where in the subplot to go\n",
    "    count = 0\n",
    "    fig   = plt.figure(figsize   = (24,20),\n",
    "                       facecolor = \"white\")\n",
    "    \n",
    "    # Enumerating allows for the list of titles to be referenced\n",
    "    for d, dataframe in enumerate(dataframes):\n",
    "        \n",
    "        # Updating the location\n",
    "        count += 1\n",
    "        ax    = fig.add_subplot(2, 2, count)\n",
    "        \n",
    "        # Creating the graph\n",
    "        sns.barplot(x       = 0,\n",
    "                    y       = dataframe.index,\n",
    "                    data    = dataframe,\n",
    "                    palette = \"deep\")\n",
    "        \n",
    "        # Setting the graph parameters\n",
    "        plt.title(f\"Most Common Words From {titles[d]}\", size = 20)\n",
    "        plt.xlabel(\"Number Of Occurences\", size = 18)\n",
    "        plt.ylabel(\"Word\", size = 18)\n",
    "        plt.xticks(size = 16)\n",
    "        plt.yticks(size = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the count vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Masking the vectorizer with English stop words\n",
    "\n",
    "cvec_cooking     = CountVectorizer(stop_words = \"english\")\n",
    "cvec_askculinary = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "# Subsetting the dataframe\n",
    "\n",
    "cooking     = model_data[model_data[\"target\"] == 1]\n",
    "askculinary = model_data[model_data[\"target\"] == 0]\n",
    "\n",
    "# Fit-transforming the vectorizer\n",
    "\n",
    "vec_cooking     = cvec_cooking.fit_transform(cooking[\"text\"])\n",
    "vec_askculinary = cvec_askculinary.fit_transform(askculinary[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vectorized dfs to a new dataframe\n",
    "\n",
    "cooking_vectorized     = pd.DataFrame(vec_cooking.toarray(), \n",
    "                                      columns = cvec_cooking.get_feature_names())\n",
    "\n",
    "askculinary_vectorized = pd.DataFrame(vec_askculinary.toarray(), \n",
    "                                      columns = cvec_askculinary.get_feature_names())\n",
    "\n",
    "# Getting the 15 most frequent words from each\n",
    "\n",
    "vectorized_cooking     = pd.DataFrame(cooking_vectorized.sum().sort_values(ascending = False).head(15))\n",
    "vectorized_askculinary = pd.DataFrame(askculinary_vectorized.sum().sort_values(ascending = False).head(15))\n",
    "\n",
    "# Plotting the most common words\n",
    "\n",
    "plot_most_frequent_words(dataframes = [vectorized_cooking, vectorized_askculinary],\n",
    "                         titles     = [\"r/Cooking\", \"r/AskCulinary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of words that occur in both subreddits.  We decided that because of that, we should create a list of customized stop words.  Furthermore, we noticed that we have to lemmatize or stem the text columns because of there are multiple forms of words in the most frequent words such as 'make' & 'making' or 'recipe' and recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the default stopwords\n",
    "\n",
    "nltk.download(\"stopwords\");\n",
    "\n",
    "# Adding our stopwords to the English set\n",
    "\n",
    "new_stopwords = [\"like\", \"just\", \"make\", \"cook\",\n",
    "                 \"use\", \"chicken\", \"recipe\", \"sauce\"]\n",
    "\n",
    "stopwords     = stopwords.words('english')\n",
    "\n",
    "stopwords.extend(new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We felt that lemmatizing is a better option than stemming because the lemma form of a word is more likely to result in an actual word of English than trying to find a word's stem: there are so many irregularities in English that it is not always easy to find the stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the lemmatizier and tokenizer\n",
    "# The tokenizer will only keep text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer  = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Setting up the lemmatizer\n",
    "\n",
    "lemmatized_posts = []\n",
    "\n",
    "for post in model_data[\"text\"]:\n",
    "    tokens = tokenizer.tokenize(post)\n",
    "    post   = [lemmatizer.lemmatize(post) for post in tokens]\n",
    "    lemmatized_posts.append(\" \".join(post))\n",
    "    \n",
    "# Appending the lemmatized posts to the dataframe\n",
    "\n",
    "model_data[\"lemmatized_text\"] = lemmatized_posts\n",
    "\n",
    "# Checking the head of the dataframe\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While checking the results from the cell above, we noticed that in `lemmatized_text` there are some URLs which need to be removed.  We used a regular expression to remove all URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[\"lemmatized_text\"] = model_data[\"lemmatized_text\"].str.replace(\"http\\S+\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline in classification gives us an idea of how exactly the model is performing.  The baseline is simply the percentage of occurrences of our target in the data as a whole.  In this case it will be what percentage of posts are from r/Cooking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model has an accuracy of >41.44% we know that it is better than simply guessing the class of a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(model_data[\"target\"].value_counts(normalize = True)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is in the format we want, we can begin the process of modeling.\n",
    "\n",
    "There are a few steps we have to do before we start running models: we have to define the X and y variables and run a train-test split on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting The X & y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data[\"lemmatized_text\"]\n",
    "y = model_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running A Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A train-test split is important because it allows us to reserve a portion of our data for test so that the model does not see all data before predicting.  In this case we want to preserve the class split, so we will stratify the data to match the distribution of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random state ensures reproducability\n",
    "# The stratify argument preserves the distribution of classes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the three models we will use will be gridsearched so that we can experiment with different combinations of hyperparameters (parameters we have to define).  Additionally, each model will be fit with a count vectorizer and a TFIDF (Term Frequency-Inverse Document Frequency) vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix allows us to look at how our model actually classified our data.  It plots the true y values and the predicted y values so that we can have an idea of how the model performs with each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We converted the confusion matrix to a dataframe to make it easier to read\n",
    "\n",
    "def create_confusion_matrix(y, y_preds):\n",
    "    cm     = confusion_matrix(y, y_preds)\n",
    "    matrix = pd.DataFrame(cm, \n",
    "                          columns = [\"Predicted r/Cooking\", \"Predicted r/AskCulinary\"], \n",
    "                          index   = [\"Actual r/Cooking\", \"Actual r/AskCulinary\"])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the evaluation metrics we want to use is specificty which is not a function we can import from sklearn.  In order to calculate this score, we will create a function based off of the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function creates a confusion matrix\n",
    "# and then calculates the specifity from\n",
    "# specific cells\n",
    "\n",
    "def calc_specificity(y, y_hat):\n",
    "    cm          = confusion_matrix(y, y_hat)\n",
    "    specificity = cm[1,1] / (cm[0,1] + cm[1, 0])\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model we will calculate an ROC-AUC score.  The ROC (receiver operating characteristic) shows us a binary classification model's ability to distinguish between two classes.  The curve, which will be plotted for our best model, shows us the distribution of the two classes.  The AUC (area under the curve) is how we actually measure the distribution of the classes: 0.5 is the lowest possible and 1.0 is the highest.\n",
    "\n",
    "This image from [GreyAtom](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152) illustrates the AUC-ROC well:\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.8 0.9.png\" alt = \"high auc_roc scores\" height = \"350\" width = \"350\">\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.5 0.7.png\" alt = \"low auc_roc scores\" height = \"350\" width = \"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not the most informative metric because we want to know how well the model is actually performing on both classes.  In order to do that we decided to look at five metrics in addition to general accuracy.\n",
    "\n",
    " \n",
    "| Metric                | Definition                                                     | Scale    |\n",
    "|:----------------------|:---------------------------------------------------------------|:---------|\n",
    "| **Accuracy**          | How well the model performed                                   | 0 to 1   | \n",
    "| **Balanced Accuracy** | The average of the sensitivity on each class                   | 0 to 1   | \n",
    "| **Specificity**       | How many positive predictions are correct                      | 0 to 1   | \n",
    "| **Sensitivity**       | How many negatives are actually correct (also known as recall) | 0 to 1   | \n",
    "| **F1 Score**          | Accuracy that takes into account the precision & recall        | 0 to 1   | \n",
    "| **ROC-AUC Score**     | A measure of the model's ability to distinguish classes        | 0.5 to 1 |\n",
    "\n",
    "While we are taking a holistic approach to evaluation, we believe our most import metric is the ROC-AUC score because it shows us how much our classes overlap; ideally our ROC-AUC scores will be as close to 1.0 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the \"classification\" report\n",
    "\n",
    "def generate_model_eval(y, y_hat):\n",
    "    print(f\"The accuracy score is         : {round(accuracy_score(y, y_hat), 5)}\")\n",
    "    print(f\"The balanced accuracy score is: {round(balanced_accuracy_score(y, y_hat), 5)}\")\n",
    "    print(f\"The specificity score is      : {round(calc_specificity(y, y_hat), 5)}\")\n",
    "    print(f\"The sensitivity score is      : {round(recall_score(y, y_hat), 5)}\")\n",
    "    print(f\"The F1 score is               : {round(f1_score(y, y_hat), 5)}\")\n",
    "    print(f\"The ROC-AUC score is          : {round(roc_auc_score(y, y_hat), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is very similar to the linear regression, but it uses a logit function to bend the line so that it can predict either 0 or 1.\n",
    "\n",
    "\n",
    "The gridsearch will be searching hyperparameters for the count vectorizer and the TFIDF, not the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_lr_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"log_reg\", LogisticRegression())])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\": [125], \n",
    "                    \"cvec__ngram_range\" : [(1,2)], \n",
    "                    \"cvec__stop_words\"  : [None]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_lr_gs = GridSearchCV(cvec_lr_pipe, \n",
    "                          param_grid = cvec_pipe_params, \n",
    "                          cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "cvec_lr_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_lr_train_preds = cvec_lr_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "cvec_lr_preds       = cvec_lr_gs.predict(X_test)\n",
    "cvec_lr_proba       = cvec_lr_gs.predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, cvec_lr_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, cvec_lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the scores are poor, although we found it surprising at how consistent they are.  We had expected a degree of overfitting due to the type of model: linear models are simple and tend to high bias.\n",
    "\n",
    "Our biggest concern is the ROC-AUC score: the scores are close to the lowest possible score and thus the model is not doing a very good job at distinguishing between the posts from r/Cooking and r/AskCulinary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "create_confusion_matrix(y_test, cvec_lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix contains the following:\n",
    "\n",
    "\n",
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|:--------------------|:------------------:|:------------------:|\n",
    "| **Actual Positive** | True Positive      | False Negative     |\n",
    "| **Actual Negative** | False Positive     | True Negative      |\n",
    "\n",
    "\n",
    "Based on the matrix, it is easy to see that the model is better at predicting posts from r/Cooking than from r/AskCulinary.  We found that surprising because there are more posts from r/AskCulinary so we expected the opposite of these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_lr_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"log_reg\", LogisticRegression())])\n",
    "\n",
    "# Setting TFIDF pipe parameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\": [650], \n",
    "                    \"tvec__ngram_range\" : [(1,1)], \n",
    "                    \"tvec__stop_words\"  : [None]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_lr_gs = GridSearchCV(tvec_lr_pipe, \n",
    "                          param_grid = tvec_pipe_params, \n",
    "                          cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "tvec_lr_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_lr_train_preds = tvec_lr_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "tvec_lr_preds       = tvec_lr_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, tvec_lr_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, tvec_lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model with the TFIDF vectorizer performed a bit better than the same model with count vectorization.  That being said, this model is more overfit.  Additionally, the specificity of >1 does not make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "create_confusion_matrix(y_test, tvec_lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the confusion matrix for the CVEC predictions, the number of true positives and true negatives have increased while the numbers of false positives and false negatives have decreased as well meaning that the model's predictive power has increased; this reflects what we saw with the model's metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector machine (in this case a classifier) is at its core a linear model.  However, instead of running like a logistic regression, it seeks to linearly separate the data.  To do that, it uses a kernel to raise the data into _n_-dimensional space.  It then uses a line, plane (3 dimensional line), or hyperplane (>3 dimensional line) to delineate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_svc_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"svc\", SVC())])\n",
    "\n",
    "# Setting TFIDF pipe parameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\": [319], \n",
    "                    \"cvec__ngram_range\" : [(1,2)], \n",
    "                    \"cvec__stop_words\"  : [None],\n",
    "                    \"svc__C\"            : [1.0],\n",
    "                    \"svc__kernel\"       : [\"rbf\"],\n",
    "                    \"svc__gamma\"        : [\"auto\"]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_svc_gs = GridSearchCV(cvec_svc_pipe, \n",
    "                           param_grid = cvec_pipe_params, \n",
    "                           cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "cvec_svc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_svc_train_preds = cvec_svc_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "cvec_svc_preds       = cvec_svc_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, cvec_svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, cvec_svc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were suprised that a support vector machine performed worse than the logistic regression: we thought that because the model seeks to make the data linearly separable as possile that it would do a better job of distinguishing the classes.  And while it out-performed the CVEC logistic regression, it was 0.03 worse than the TVEC logist regression.  Furthermore, this model is much more overfit: the specificity for example dropped by 0.344."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "create_confusion_matrix(y_test, cvec_svc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model predicts the r/Cooking more than the r/AskCulinary which again we found surprising.  While it did a better job with the true positives, the model overall is worse because the false negatives increased while the true negatives decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_svc_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"svc\", SVC())])\n",
    "\n",
    "# Setting TFIDF pipe parameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\": [1], \n",
    "                    \"tvec__ngram_range\" : [(1,1)], \n",
    "                    \"tvec__stop_words\"  : [None],\n",
    "                    \"svc__C\"            : [1.0],\n",
    "                    \"svc__kernel\"       : [\"rbf\"],\n",
    "                    \"svc__gamma\"        : [\"auto\"]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_svc_gs = GridSearchCV(tvec_svc_pipe, \n",
    "                           param_grid = tvec_pipe_params, \n",
    "                           cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "tvec_svc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_svc_train_preds = tvec_svc_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "tvec_svc_preds       = tvec_svc_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, tvec_svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, tvec_svc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the performance of the SVC with count vectorization, we were surprised by how much more poorly this model performed: scores in the range of 0.2 to 0.4 are simply unacceptable.  We expected these scores to be slightly higher than the count vectorized scores because in the case of the logistic regression the scores improved with TFIDF vectorization.  Nevertheless, the model is is not overfit at all and in a few cases performed slightly better on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "create_confusion_matrix(y_test, tvec_svc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is dramatically different than the previous matrix.  While the true positives have increased and the false negatives decreased, the number of true negatives decrease and the false positives increased as well.  We found it interesting that the number of true positives have continually increased while the others have been so variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest classifier is a decision tree based classification method.  However, it has advantages over other tree based models.  Firstly, it bootstraps the dataframe to have a random subset of the data, but it also takes a random subset of the features.  Having two levels of randomness in the model reduce the likelihood of the model being overfit on training data but it also allows the model to be less prone to variance caused by a large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_rf_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"rf\", RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\"   : [1000], \n",
    "                    \"cvec__ngram_range\"    : [(1,1)], \n",
    "                    \"cvec__stop_words\"     : [None],\n",
    "                    \"rf__n_estimators\"     : [72],\n",
    "                    \"rf__min_samples_split\": [6],\n",
    "                    \"rf__min_samples_leaf\" : [2],\n",
    "                    \"rf__max_depth\"        : [20]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_rf_gs = GridSearchCV(cvec_rf_pipe, \n",
    "                          param_grid = cvec_pipe_params, \n",
    "                          cv         = 5,\n",
    "                          n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "cvec_rf_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_rf_train_preds = cvec_rf_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "cvec_rf_preds       = cvec_rf_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, cvec_rf_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, cvec_rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random forest model is significantly overfit, which we did not expect because the random forest's two-level randomness is designed to help prevent overfitting; overall this model performed poorly.  Additionally, we are not sure how the specificity is >1 here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "create_confusion_matrix(y_test, cvec_rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the true positives and negatives increased while the false negatives and positives decreased.  This is an improvement over the previous model, but it is still not great by any means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_rf_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"rf\", RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\"   : [250], \n",
    "                    \"tvec__ngram_range\"    : [(1,2)], \n",
    "                    \"tvec__stop_words\"     : [None],\n",
    "                    \"rf__n_estimators\"     : [30],\n",
    "                    \"rf__min_samples_split\": [6],\n",
    "                    \"rf__min_samples_leaf\" : [2],\n",
    "                    \"rf__max_depth\"        : [12]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_rf_gs = GridSearchCV(tvec_rf_pipe, \n",
    "                          param_grid = tvec_pipe_params, \n",
    "                          cv         = 5,\n",
    "                          n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "tvec_rf_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_rf_train_preds = tvec_rf_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "tvec_rf_preds       = tvec_rf_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, tvec_rf_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, tvec_rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this model is very overfit and performed poorly.  We also do not understand how the specificity is again >1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "create_confusion_matrix(y_test, tvec_rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly here the false positives and true negatives were the same while the true positives decreased and false negatives increased: the models have continually done better at predicting the positive class (posts from r/Cooking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a tree-based boosting model that iteratively fits tree models on the errors of the previous model and uses gradient descent to help minimize the loss function.  Furthermore, the XGBoost is much more computationally effecient and can be parallelized unlike orther boosting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_xgbc_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                           (\"xgbc\", XGBClassifier(n_jobs                = 6,\n",
    "                                                  early_stopping_rounds = 10))])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\"   : [200], \n",
    "                    \"cvec__ngram_range\"    : [(1,3)], \n",
    "                    \"cvec__stop_words\"     : [None],\n",
    "                    \"xgbc__max_depth\"      : [3],\n",
    "                    \"xgbc__learning_rate\"  : [0.04],\n",
    "                    \"xgbc__n_estimators\"   : [175],\n",
    "                    \"xgbc__gamma\"          : [3.0]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_xgbc_gs = GridSearchCV(cvec_xgbc_pipe, \n",
    "                            param_grid = cvec_pipe_params, \n",
    "                            cv         = 5,\n",
    "                            n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "cvec_xgbc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_xgbc_train_preds = cvec_xgbc_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "cvec_xgbc_preds       = cvec_xgbc_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, cvec_xgbc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, cvec_xgbc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had expected the XGBoost classifier to be the best model because it is a boosting model: it fits models on the errors from each model it fits.  This model is bad not only because the test scores very low, but because of how overfit the models are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "create_confusion_matrix(y_test, cvec_xgbc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrices have been variable over previous models and this one is no different.  The true and false positives decreased while the false negatives and true negatives increased.  The trend of predicting more r/Cooking posts is present here as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_xgbc_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                           (\"xgbc\", XGBClassifier(n_jobs                = 6,\n",
    "                                                  seed                  = 42,\n",
    "                                                  early_stopping_rounds = 10))])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\"   : [525], \n",
    "                    \"tvec__ngram_range\"    : [(1,3)], \n",
    "                    \"tvec__stop_words\"     : [stopwords],\n",
    "                    \"xgbc__max_depth\"      : [3],\n",
    "                    \"xgbc__learning_rate\"  : [0.25],\n",
    "                    \"xgbc__n_estimators\"   : [139],\n",
    "                    \"xgbc__gamma\"          : [1.0]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_xgbc_gs = GridSearchCV(tvec_xgbc_pipe, \n",
    "                            param_grid = tvec_pipe_params, \n",
    "                            cv         = 5,\n",
    "                            n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "tvec_xgbc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_xgbc_train_preds = tvec_xgbc_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "tvec_xgbc_preds       = tvec_xgbc_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, tvec_xgbc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_test, tvec_xgbc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we were surprised by how poorly the XGBoost model did for the same reasons: it is extremely overfit and its metric scores are poor.  Again the specificity is >1 which idoes not make sense to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "create_confusion_matrix(y_test, tvec_xgbc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is dramatically worse than the previous: both cases of negatives rose a significant amount, while the positives dropped.  Still, the model predicted more postives than negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the best model given the metric scores, especially the ROC-AUC score, is the logistc regression.  In particular, the logistic regression model with TFIDF is the best overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar plot of a model's scores\n",
    "\n",
    "def plot_scores(df, column, label):\n",
    "    \n",
    "    # Setting the figure size\n",
    "    plt.figure(figsize   = (15,5),\n",
    "               facecolor = \"white\")\n",
    "    \n",
    "    # Plotting the bar plot\n",
    "    sns.barplot(x    = df.index,\n",
    "                y    = column,\n",
    "                data = df)\n",
    "    \n",
    "    # Setting the baseline line\n",
    "    plt.axhline(41.8898, \n",
    "                color = \"black\")\n",
    "    \n",
    "    # Setting graph parameters\n",
    "    plt.title(label, size = 20)\n",
    "    plt.xlabel(\"Model\", size = 18)\n",
    "    plt.ylabel(\"Score\", size = 18)\n",
    "    plt.xticks(size  = 14)\n",
    "    plt.yticks(ticks = np.arange(0,110,10), \n",
    "               size  = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(model_prob, X_test, y_test, y_pred, title):\n",
    "    \n",
    "    # Calculating probabilities\n",
    "    model_prob    = [i[1] for i in model_prob.predict_proba(X_test)]\n",
    "    \n",
    "    # Creating a dataframeout of the true values & probas\n",
    "    model_pred_df = pd.DataFrame({\"true_values\": y_test,\n",
    "                                  \"pred_probs\" : model_prob})\n",
    "\n",
    "    # Setting threshold values    \n",
    "    thresholds = np.linspace(0, 1, 500) \n",
    "    \n",
    "    # Calculating the sensitivity\n",
    "    def true_positive_rate(df, true_col, pred_prob_col, threshold):\n",
    "        true_positive  = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "        false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "        return true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    # Calculating the false positives\n",
    "    def false_positive_rate(df, true_col, pred_prob_col, threshold):\n",
    "        true_negative  = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "        false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "        return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "    # Calculating the sensitivity and false positives for each point in the threhold\n",
    "    tpr_values = [true_positive_rate(model_pred_df, \"true_values\", \"pred_probs\", prob) for prob in thresholds]\n",
    "    fpr_values = [false_positive_rate(model_pred_df, 'true_values', \"pred_probs\", prob) for prob in thresholds]\n",
    "\n",
    "    # Setting up the graph\n",
    "    plt.figure(figsize   = (13,7),\n",
    "               facecolor = \"white\")\n",
    "    \n",
    "    # Plotting the predicted\n",
    "    plt.plot(fpr_values, \n",
    "             tpr_values,\n",
    "             color = \"darkorange\",\n",
    "             label = \"ROC Curve\")\n",
    "    \n",
    "    # Setting the baseline\n",
    "    plt.plot(np.linspace(0, 1, 500),\n",
    "             np.linspace(0, 1, 500),\n",
    "             color     = \"darkblue\",\n",
    "             label     = \"Baseline\"),\n",
    "    \n",
    "    # Setting model parameters\n",
    "    plt.title(title, fontsize = 18)\n",
    "    plt.ylabel(\"Sensitivity\", size = 16)\n",
    "    plt.xlabel(\"1 - Specificity\", size = 16)\n",
    "    plt.xticks(size = 14)\n",
    "    plt.yticks(size = 14)\n",
    "    plt.legend(bbox_to_anchor = (1.04,1), \n",
    "               loc            = \"upper left\",\n",
    "               fontsize       = 16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# The code was modified from code written by Matt Brems \n",
    "# during our lesson on classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline accuracy\n",
    "\n",
    "baseline = [0.418898]\n",
    "\n",
    "# Count vectorizer metrics\n",
    "\n",
    "cvec_accuracy          = [0.65683, 0.66544, 0.66421, 0.64945]\n",
    "cvec_balanced_accuracy = [0.63445, 0.63617, 0.61924, 0.61548]\n",
    "cvec_specificity       = [0.60573, 0.56985, 0.42491, 0.48421]\n",
    "cvec_sensitivity       = [0.4956,  0.45455, 0.34018, 0.40469]\n",
    "cvec_f1_score          = [0.54781, 0.53265, 0.45941, 0.49198]\n",
    "cvec_rocauc_score      = [0.63445, 0.63617, 0.61924, 0.61548]\n",
    "\n",
    "# TFIDF vectorizer metrics\n",
    "\n",
    "tvec_accuracy          = [0.68758, 0.60271, 0.64084, 0.65314]\n",
    "tvec_balanced_accuracy = [0.66419, 0.55244, 0.59911, 0.63209]\n",
    "tvec_specificity       = [0.69685, 0.25387, 0.39726, 0.60638]\n",
    "tvec_sensitivity       = [0.51906, 0.24047, 0.34018, 0.50147]\n",
    "tvec_f1_score          = [0.58224, 0.33676, 0.44275, 0.54808]\n",
    "tvec_rocauc_score      = [0.66419, 0.55244, 0.59911, 0.63209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_scores = pd.DataFrame(data    = [cvec_accuracy, cvec_balanced_accuracy, \n",
    "                                      cvec_specificity,cvec_sensitivity, cvec_f1_score],\n",
    "                           columns = [\"Log. Reg.\", \"SVC\", \"Random Forest\", \"XGBoost\"],\n",
    "                           index   = [\"Accuracy\", \"Balanced Accuracy\", \n",
    "                                      \"Specificity\", \"Sensitivity\", \"F1 Score\"])\n",
    "\n",
    "\n",
    "tvec_scores = pd.DataFrame(data    = [tvec_accuracy, tvec_balanced_accuracy, \n",
    "                                      tvec_specificity, tvec_sensitivity, tvec_f1_score,],\n",
    "                           columns = [\"Log. Reg.\", \"SVC\", \"Random Forest\", \"XGBoost\"],\n",
    "                           index   = [\"Accuracy\", \"Balanced Accuracy\", \n",
    "                                      \"Specificity\", \"Sensitivity\", \"F1 Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(df       = tvec_scores*100,\n",
    "            column   = \"Log. Reg.\",\n",
    "            label    = \"Logistic Regression - TVEC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal black line represents the baseline accuracy which is the percentage of the data from the positive class, i.e. r/Cooking.  If the two accuracy metrics were less than the baseline accuracy it would indicate the model performs worse than randomly guessing 41.44% of the data is from r/Cooking.  However, our two accuracy metrics are greater than that percentage so they out performed the baseline.  That being said, the scores are not good and we cannot say that this model is accurate even though the it performed better than the other seven we ran.  Despite that, it is a good sign that the accuracy and balanced accuracy are close to each other.\n",
    "\n",
    "\n",
    "As mentioned above, `specificity` is the how many positive predictions were correct and `Sensitivity` is how many negative predictions are correct.  As we said with the confusion matrices, all of the models were better at predicting the positive class which is easily seen here.\n",
    "\n",
    "`F1 Score` is an accuracy measurement that takes into account the specificity and sensitivity.  The score is low, ~58%, which makes it a little difficult to interpret because it can be difficult if the low score is caused by specificity or sensitivity.  However, we know from the confusion matrix that the score is low because of the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc_score = round(roc_auc_score(y_test, tvec_lr_preds), 5)\n",
    "\n",
    "roc(model_prob = tvec_lr_gs,\n",
    "    X_test     = X_test,\n",
    "    y_test     = y_test,\n",
    "    y_pred     = tvec_lr_preds,\n",
    "    title      = f\"ROC For TVEC Logistc Regression With A Score Of {rocauc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is a representation of the relationship between the false positives (1 - specificity) and the true negatives.  We can see that the as the number of false positives increases the grows but not in a linear fashion.\n",
    "\n",
    "The low score indicates that the model is not very good at predicting the two classes, which we also saw above.  Furthermore, the curve is close to the baseline; the baseline being the point at which guess which post came from which subreddit.  Thus we hoped to have our curve be as far from the baseline as possible, ideally a 90 angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Table-Of-Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
