{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas                        as pd\n",
    "import numpy                         as np\n",
    "import seaborn                       as sns\n",
    "import matplotlib.pyplot             as plt\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.metrics                 import confusion_matrix\n",
    "from sklearn.metrics                 import roc_auc_score\n",
    "from sklearn.metrics                 import accuracy_score\n",
    "from sklearn.metrics                 import precision_score\n",
    "from sklearn.metrics                 import recall_score\n",
    "from sklearn.metrics                 import f1_score\n",
    "from sklearn.metrics                 import balanced_accuracy_score\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.svm                     import SVC\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.stem                       import WordNetLemmatizer\n",
    "from nltk.tokenize                   import RegexpTokenizer \n",
    "from xgboost                         import XGBClassifier\n",
    "from IPython.core.display            import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "-------\n",
    "\n",
    "1. [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Overview](#Overview)\n",
    "    - [Visuals](#Visuals)\n",
    "    \n",
    "    \n",
    "2. [Lemmatizing](#Lemmatizing)\n",
    "\n",
    "\n",
    "3. [Establishing The Baseline](#Establishing-The-Baseline)\n",
    "\n",
    "\n",
    "4. [Modeling](#Modeling)\n",
    "    - [Setting The X and y variables](#Setting-The-X-and-y-variables)\n",
    "    - [Running A Train-Test Split](#Running-A-Train-Test-Split)\n",
    "    - [Evaluation Formulae](#Evaluation-Formulae)\n",
    "    - [Logistic Regression](#Logistic-Regression)\n",
    "    - [Random Forest Classifier](#Random-Forest-Classifier)\n",
    "    - [Support Vector Classifier](#Support-Vector-Classifier)\n",
    "    - [XGboost Classifier](#XGBoost-Classifier)\n",
    "    \n",
    "    \n",
    "5. [Model Evaluation](#Model-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.read_csv(\"../Data/model_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data's head\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "model_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types\n",
    "\n",
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_length_dist(text_list):\n",
    "    plt.figure(figsize = (18,6))\n",
    "    sns.distplot(text_list, kde = False, color = \"black\",\n",
    "                 bins = 60)\n",
    "    plt.title(f\"Distribution Of Text Lengths\", size = 18)\n",
    "    plt.xlabel(\"Length\", size = 16)\n",
    "    plt.ylabel(\"Frequency\", size = 16)\n",
    "    plt.xticks(np.arange(0,23500,1500), size = 14)\n",
    "    plt.yticks(size = 14)\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_frequent_authors(df, col):\n",
    "    \n",
    "    plt.figure(figsize = (20,6))\n",
    "    sns.barplot(x = df.index,\n",
    "                y = col,\n",
    "                data = df)\n",
    "    plt.title(\"Most Common Posters\", size = 18)\n",
    "    plt.xlabel(\"Reddit User\", size = 16)\n",
    "    plt.ylabel(\"Number Of Posts\", size = 16)\n",
    "    plt.xticks(size = 13)\n",
    "    plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a list of text lengths\n",
    "\n",
    "length_list = [len(text) for text in model_data[\"text\"]]\n",
    "\n",
    "plot_text_length_dist(length_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the posts are relatively short (<2000 words), but there are a few that are extremely long (>20,000 words.)  We expected that most posts would be less than a few thousand words, which is true for the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Most Frequent Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_count = pd.DataFrame(model_data[\"author\"].value_counts().head(10))\n",
    "\n",
    "plot_most_frequent_authors(df  = author_count, \n",
    "                           col = \"author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not really know what to expect when we plotted this graph, because it is generally the case that a few users post most frequently and most barely post at all.  We would have like to look at the number of comments by each user in both subreddits as a measure of activity, but that is beyond the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subreddit Of Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels = [\"r/Cooking\", \"r/AskCulinary\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(model_data[\"source\"])\n",
    "plt.title(\"Post Origin\", size = 18)\n",
    "plt.xlabel(\"Source\", size = 16)\n",
    "plt.ylabel(\"Number Of Posts\", size = 16)\n",
    "plt.xticks(np.arange(0,2,1), \n",
    "           labels = tick_labels, \n",
    "           size = 14)\n",
    "plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were a little surprised that there are more r/AskCulinary posts because we had roughly equal numbers of pulls from each subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Most Common Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start modeling, we need to know what the most frequent words are in each subreddit are because it might be harder for our model to predict with those words in the dataframe.\n",
    "\n",
    "We will subset the data frame into posts from r/Cooking and r/AskCulinary and use count vectorizer to determine the most frequent words.  We will also remove stop words from the outset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_frequent_words(dataframes, titles):\n",
    "    count = 0\n",
    "    fig   = plt.figure(figsize = (24,20))\n",
    "    for d, dataframe in enumerate(dataframes):\n",
    "        count += 1\n",
    "        ax    = fig.add_subplot(2, 2, count)\n",
    "        sns.barplot(x       = 0,\n",
    "                    y       = dataframe.index,\n",
    "                    data    = dataframe,\n",
    "                    palette = \"deep\")\n",
    "        plt.title(f\"Most Common Words From {titles[d]}\", size = 20)\n",
    "        plt.xlabel(\"Number Of Occurences\", size = 18)\n",
    "        plt.ylabel(\"Word\", size = 18)\n",
    "        plt.xticks(size = 16)\n",
    "        plt.yticks(size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the count vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Masking the vectorizer with English stop words\n",
    "\n",
    "cvec_cooking     = CountVectorizer(stop_words = \"english\")\n",
    "cvec_askculinary = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "# Subsetting the dataframe\n",
    "\n",
    "cooking     = model_data[model_data[\"target\"] == 1]\n",
    "askculinary = model_data[model_data[\"target\"] == 0]\n",
    "\n",
    "# Fit-transforming the vectorizer\n",
    "\n",
    "vec_cooking     = cvec_cooking.fit_transform(cooking[\"text\"])\n",
    "vec_askculinary = cvec_askculinary.fit_transform(askculinary[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vectorized dfs to a new dataframe\n",
    "\n",
    "cooking_vectorized = pd.DataFrame(vec_cooking.toarray(), \n",
    "                                  columns = cvec_cooking.get_feature_names())\n",
    "\n",
    "askculinary_vectorized = pd.DataFrame(vec_askculinary.toarray(), \n",
    "                                      columns = cvec_askculinary.get_feature_names())\n",
    "\n",
    "# Getting the 15 most frequent words from each\n",
    "\n",
    "vectorized_cooking     = pd.DataFrame(cooking_vectorized.sum().sort_values(ascending = False).head(15))\n",
    "vectorized_askculinary = pd.DataFrame(askculinary_vectorized.sum().sort_values(ascending = False).head(15))\n",
    "\n",
    "# Plotting the most common words\n",
    "\n",
    "plot_most_frequent_words(dataframes = [vectorized_cooking, vectorized_askculinary],\n",
    "                         titles     = [\"r/Cooking\", \"r/AskCulinary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of words that occur in both subreddits.  We decided that because of that, we should create a list of customized stop words.  Furthermore, we noticed that we have to lemmatize or stem the text columns because of there are multiple forms of words in the most frequent words such as 'make' & 'making' or 'recipe' and recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the default stopwords\n",
    "\n",
    "nltk.download(\"stopwords\");\n",
    "\n",
    "# Adding our stopwords to the English set\n",
    "\n",
    "new_stopwords = [\"like\", \"just\", \"make\", \"cook\",\n",
    "                 \"use\", \"chicken\", \"recipe\", \"sauce\"]\n",
    "\n",
    "stopwords     = stopwords.words('english')\n",
    "\n",
    "stopwords.extend(new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We felt that lemmatizing is a better option than stemming because the lemma form of a word is more likely to result in an actual word of English than trying to find a word's stem: there are so many irregularities in English that it is not always easy to find the stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the lemmatizier and tokenizer\n",
    "# The tokenizer will only keep text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer  = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Setting up the lemmatizer\n",
    "\n",
    "lemmatized_posts = []\n",
    "\n",
    "for post in model_data[\"text\"]:\n",
    "    tokens = tokenizer.tokenize(post)\n",
    "    post  = [lemmatizer.lemmatize(post) for post in tokens]\n",
    "    lemmatized_posts.append(\" \".join(post))\n",
    "    \n",
    "# Appending the lemmatized posts to the dataframe\n",
    "\n",
    "model_data[\"lemmatized_text\"] = lemmatized_posts\n",
    "\n",
    "# Checking the head of the dataframe\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While checking the results from the cell above, we noticed that in `lemmatized_text` there are some URLs which need to be removed.  We used a regular expression to remove all URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[\"lemmatized_text\"] = model_data[\"lemmatized_text\"].str.replace(\"http\\S+\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline in classification gives us an idea of how exactly the model is performing.  The baseline is simply the percentage of occurrences of our target in the data as a whole.  In this case it will be what percentage of posts are from r/Cooking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model has an accuracy of >41.44% we know that it is better than simply guessing the class of a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(model_data[\"target\"].value_counts(normalize = True)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is in the format we want, we can begin the process of modeling.\n",
    "\n",
    "There are a few steps we have to do before we start running models: we have to define the X and y variables and run a train-test split on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting The X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data[\"lemmatized_text\"]\n",
    "y = model_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running A Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A train-test split is important because it allows us to reserve a portion of our data for test so that the model does not see all data before predicting.  In this case we want to preserve the class split, so we will stratify the data to match the distribution of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stratify argument preserves the distribution of classes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the three models we will use will be gridsearched so that we can experiment with different combinations of hyperparameters (parameters we have to define).  Additionally, each model will be fit with a count vectorizer and a TFIDF (Term Frequency-Inverse Document Frequency) vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix allows us to look at how our model actually classified our data.  It plots the true y values and the predicted y values so that we can have an idea of how the model performs with each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a confusion matrix\n",
    "# We convert the confusion matrix to a dataframe\n",
    "# to make it easier to read\n",
    "\n",
    "def create_confusion_matrix(y, y_preds):\n",
    "    cm     = confusion_matrix(y, y_preds)\n",
    "    matrix = pd.DataFrame(cm, \n",
    "                          columns = [\"Predicted r/Cooking\", \"Predicted r/AskCulinary\"], \n",
    "                          index = [\"Actual r/Cooking\", \"Actual r/AskCulinary\"])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the evaluation metrics we want to use is specificty which is not a function we can import from sklearn.  In order to calculate this score, we will create a function based off of the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function creates a confusion matrix\n",
    "# and then calculates the specifity from\n",
    "# specific cells\n",
    "\n",
    "def calc_specificity(y, y_hat):\n",
    "    cm          = confusion_matrix(y, y_hat)\n",
    "    specificity = cm[1,1] / (cm[0,1] + cm[1, 0])\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model we will calculate an ROC-AUC score.  The ROC (receiver operating characteristic) shows us a binary classification model's ability to distinguish between two classes.  The curve, which will be plotted for our best model, shows us the distribution of the two classes.  The AUC (area under the curve) is how we actually measure the distribution of the classes: 0.5 is the lowest possible and 1.0 is the highest.\n",
    "\n",
    "This image from [GreyAtom](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152) illustrates the AUC-ROC well:\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.8 0.9.png\" alt = \"high auc_roc scores\" height = \"350\" width = \"350\">\n",
    "\n",
    "<img src = \"../Images/ROC_AUC 0.5 0.7.png\" alt = \"low auc_roc scores\" height = \"350\" width = \"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not the most informative metric because we want to know how well the model is actually performing on both classes.  In order to do that we decided to look at four metrics in addition to general accuracy.  The four extra are:\n",
    "\n",
    "- **Balanced Accuracy**: the average of the recall on each class\n",
    "\n",
    "\n",
    "- **Precision**: how many positives are actually correct\n",
    "\n",
    "\n",
    "- **Sensitivity**: how many negatives are actually correct (also known as recall)\n",
    "\n",
    "\n",
    "- **F1 Score**: a measure of accuracy that takes into account the precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the \"classification\" report\n",
    "\n",
    "def generate_model_eval(y, y_hat):\n",
    "    print(f\"The accuracy score is         : {round(accuracy_score(y, y_hat), 5)}\")\n",
    "    print(f\"The balanced accuracy score is: {round(balanced_accuracy_score(y, y_hat), 5)}\")\n",
    "    print(f\"The specificity score is      : {round(calc_specificity(y, y_hat), 5)}\")\n",
    "    print(f\"The recall score is           : {round(recall_score(y, y_hat), 5)}\")\n",
    "    print(f\"The F1 score is               : {round(f1_score(y, y_hat), 5)}\")\n",
    "    print(f\"The ROC-AUC score is          : {round(roc_auc_score(y, y_hat), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is very similar to the linear regression, but it uses a logit function to bend the line so that it can predict either 0 or 1.\n",
    "\n",
    "\n",
    "The gridsearch will be searching hyperparameters for the count vectorizer and the TFIDF, not the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_lr_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"log_reg\", LogisticRegression())])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\": [125], \n",
    "                    \"cvec__ngram_range\" : [(1,2)], \n",
    "                    \"cvec__stop_words\"  : [None]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_lr_gs = GridSearchCV(cvec_lr_pipe, \n",
    "                          param_grid = cvec_pipe_params, \n",
    "                          cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "cvec_lr_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_lr_train_preds = cvec_lr_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "cvec_lr_preds       = cvec_lr_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, cvec_lr_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, cvec_lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_lr_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"log_reg\", LogisticRegression())])\n",
    "\n",
    "# Setting TFIDF pipe parameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\": [650], \n",
    "                    \"tvec__ngram_range\" : [(1,1)], \n",
    "                    \"tvec__stop_words\"  : [None]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_lr_gs = GridSearchCV(tvec_lr_pipe, \n",
    "                          param_grid = tvec_pipe_params, \n",
    "                          cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "tvec_lr_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_lr_train_preds = tvec_lr_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "tvec_lr_preds       = tvec_lr_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, tvec_lr_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, tvec_lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector machine (in this case a classifier) is at its core a linear model.  However, instead of running like a logistic regression, it seeks to linearly separate the data.  To do that, it uses a kernel to raise the data into _n_-dimensional space.  It then uses a line, plane (3 dimensional line), or hyperplane (>3 dimensional line) to delineate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_svc_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"svc\", SVC())])\n",
    "\n",
    "# Setting TFIDF pipe parameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\": [319], \n",
    "                    \"cvec__ngram_range\" : [(1,2)], \n",
    "                    \"cvec__stop_words\"  : [None],\n",
    "                    \"svc__C\"            : [1.0],\n",
    "                    \"svc__kernel\"       : [\"rbf\"],\n",
    "                    \"svc__gamma\"        : [\"auto\"]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_svc_gs = GridSearchCV(cvec_svc_pipe, \n",
    "                           param_grid = cvec_pipe_params, \n",
    "                           cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "cvec_svc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_svc_train_preds = cvec_svc_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "cvec_svc_preds       = cvec_svc_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, cvec_svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, cvec_svc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_svc_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"svc\", SVC())])\n",
    "\n",
    "# Setting TFIDF pipe parameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\": [1], \n",
    "                    \"tvec__ngram_range\" : [(1,1)], \n",
    "                    \"tvec__stop_words\"  : [None],\n",
    "                    \"svc__C\"            : [1.0],\n",
    "                    \"svc__kernel\"       : [\"rbf\"],\n",
    "                    \"svc__gamma\"        : [\"auto\"]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_svc_gs = GridSearchCV(tvec_svc_pipe, \n",
    "                           param_grid = tvec_pipe_params, \n",
    "                           cv         = 5)\n",
    "\n",
    "# Fitting the training data to the pipeline model\n",
    "\n",
    "tvec_svc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_svc_train_preds = tvec_svc_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "tvec_svc_preds       = tvec_svc_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, tvec_svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, tvec_svc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest classifier is a decision tree based classification method.  However, it has advantages over other tree based models.  Firstly, it bootstraps the dataframe to have a random subset of the data, but it also takes a random subset of the features.  Having two levels of randomness in the model reduce the likelihood of the model being overfit on training data but it also allows the model to be less prone to variance caused by a large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_rf_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"rf\", RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\"   : [1000], \n",
    "                    \"cvec__ngram_range\"    : [(1,1)], \n",
    "                    \"cvec__stop_words\"     : [None],\n",
    "                    \"rf__n_estimators\"     : [72],\n",
    "                    \"rf__min_samples_split\": [6],\n",
    "                    \"rf__min_samples_leaf\" : [2],\n",
    "                    \"rf__max_depth\"        : [20]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_rf_gs = GridSearchCV(cvec_rf_pipe, \n",
    "                          param_grid = cvec_pipe_params, \n",
    "                          cv         = 5,\n",
    "                          n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "cvec_rf_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_rf_train_preds = cvec_rf_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "cvec_rf_preds       = cvec_rf_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, cvec_rf_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, cvec_rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_rf_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"rf\", RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\"   : [250], \n",
    "                    \"tvec__ngram_range\"    : [(1,2)], \n",
    "                    \"tvec__stop_words\"     : [None],\n",
    "                    \"rf__n_estimators\"     : [30],\n",
    "                    \"rf__min_samples_split\": [6],\n",
    "                    \"rf__min_samples_leaf\" : [2],\n",
    "                    \"rf__max_depth\"        : [12]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_rf_gs = GridSearchCV(tvec_rf_pipe, \n",
    "                          param_grid = tvec_pipe_params, \n",
    "                          cv         = 5,\n",
    "                          n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "tvec_rf_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_rf_train_preds = tvec_rf_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "tvec_rf_preds       = tvec_rf_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, tvec_rf_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, tvec_rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a tree-based boosting model that iteratively fits tree models on the errors of the previous model and uses gradient descent to help minimize the loss function.  Furthermore, the XGBoost is much more computationally effecient and can be parallelized unlike orther boosting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_xgbc_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                           (\"xgbc\", XGBClassifier(n_jobs                = 6,\n",
    "                                                  early_stopping_rounds = 10))])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\"   : [200], \n",
    "                    \"cvec__ngram_range\"    : [(1,3)], \n",
    "                    \"cvec__stop_words\"     : [None],\n",
    "                    \"xgbc__max_depth\"      : [3],\n",
    "                    \"xgbc__learning_rate\"  : [0.04],\n",
    "                    \"xgbc__n_estimators\"   : [175],\n",
    "                    \"xgbc__gamma\"          : [3.0]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_xgbc_gs = GridSearchCV(cvec_xgbc_pipe, \n",
    "                            param_grid = cvec_pipe_params, \n",
    "                            cv         = 5,\n",
    "                            n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "cvec_xgbc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_xgbc_train_preds = cvec_xgbc_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "cvec_xgbc_preds       = cvec_xgbc_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, cvec_xgbc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "generate_model_eval(y_test, cvec_xgbc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_xgbc_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                           (\"xgbc\", XGBClassifier(n_jobs                = 6,\n",
    "                                                  seed                  = 42,\n",
    "                                                  early_stopping_rounds = 10))])\n",
    "\n",
    "# Setting the pipeline hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\"   : [525], \n",
    "                    \"tvec__ngram_range\"    : [(1,3)], \n",
    "                    \"tvec__stop_words\"     : [stopwords],\n",
    "                    \"xgbc__max_depth\"      : [3],\n",
    "                    \"xgbc__learning_rate\"  : [0.25],\n",
    "                    \"xgbc__n_estimators\"   : [139],\n",
    "                    \"xgbc__gamma\"          : [1.0]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_xgbc_gs = GridSearchCV(tvec_xgbc_pipe, \n",
    "                            param_grid = tvec_pipe_params, \n",
    "                            cv         = 5,\n",
    "                            n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "tvec_xgbc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_xgbc_train_preds = tvec_xgbc_gs.predict(X_train)\n",
    "\n",
    "# Generating testing predictions\n",
    "\n",
    "tvec_xgbc_preds       = tvec_xgbc_gs.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_train, tvec_xgbc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "\n",
    "generate_model_eval(y_test, tvec_xgbc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer predictions\n",
    "\n",
    "cvec_accuracy          = [0.62995, 0.65718, 0.65718, 0.64975]\n",
    "cvec_balanced_accuracy = [0.59815, 0.61139, 0.60094, 0.60287]\n",
    "cvec_specificity       = [0.46154, 0.41516, 0.32852, 0.38869]\n",
    "cvec_sensitivity       = [0.41194, 0.34328, 0.27164, 0.32836]\n",
    "cvec_f1_score          = [0.48, 0.45365, 0.39651, 0.43738]\n",
    "cvec_rocauc_score      = [0.59815, 0.61139, 0.60094, 0.60287]\n",
    "\n",
    "# TFIDF vectorizer predictions\n",
    "\n",
    "tvec_accuracy          = [0.67203, 0.60767, 0.64356, 0.64109]\n",
    "tvec_balanced_accuracy = [0.6367, 0.55082, 0.58931, 0.61376]\n",
    "tvec_specificity       = [0.5434, 0.23028, 0.31597, 0.52414]\n",
    "tvec_sensitivity       = [0.42985, 0.21791, 0.27164, 0.45373]\n",
    "tvec_f1_score          = [0.5208, 0.31533, 0.38723, 0.51178]\n",
    "tvec_rocauc_score      = [0.6367, 0.55082, 0.58931, 0.61376]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_scores = pd.DataFrame(data    = [cvec_accuracy, cvec_balanced_accuracy, cvec_specificity,\n",
    "                                      cvec_sensitivity, cvec_f1_score, cvec_rocauc_score],\n",
    "                           columns = [\"Log. Reg.\", \"SVC\", \"Random Forest\", \"XGBoost\"],\n",
    "                           index   = [\"Accuracy\", \"Balanced Accuracy\", \"Specificity\",\n",
    "                                      \"Sensitivity\", \"F1 Score\", \"ROC-AUC Score\"]).T\n",
    "\n",
    "tvec_scores = pd.DataFrame(data    = [tvec_accuracy, tvec_balanced_accuracy, tvec_specificity,\n",
    "                                      tvec_sensitivity, tvec_f1_score, tvec_rocauc_score],\n",
    "                           columns = [\"Log. Reg.\", \"SVC\", \"Random Forest\", \"XGBoost\"],\n",
    "                           index   = [\"Accuracy\", \"Balanced Accuracy\", \"Specificity\",\n",
    "                                      \"Sensitivity\", \"F1 Score\", \"ROC-AUC Score\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
