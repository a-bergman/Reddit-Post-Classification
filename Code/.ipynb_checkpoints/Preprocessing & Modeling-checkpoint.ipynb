{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                        as pd\n",
    "import numpy                         as np\n",
    "import seaborn                       as sns\n",
    "import matplotlib.pyplot             as plt\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.metrics                 import confusion_matrix\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.neighbors               import KNeighborsClassifier\n",
    "from nltk.stem                       import WordNetLemmatizer\n",
    "from nltk.tokenize                   import word_tokenize \n",
    "from IPython.core.display            import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "-------\n",
    "\n",
    "1. [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Overview](#Overview)\n",
    "    - [Visuals](#Visuals)\n",
    "    \n",
    "    \n",
    "2. [Preprocessing](#Preprocessing)\n",
    "\n",
    "\n",
    "3. [Establishing The Baseline](#Establishing-The-Baseline)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.read_csv(\"../Data/model_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data's head\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "model_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types\n",
    "\n",
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_length_dist(list):\n",
    "    plt.figure(figsize = (18,6))\n",
    "    sns.distplot(list, kde = False, color = \"black\",\n",
    "                 bins = 60)\n",
    "    plt.title(f\"Distribution Of Text Lengths\", size = 18)\n",
    "    plt.xlabel(\"Length\", size = 16)\n",
    "    plt.ylabel(\"Frequency\", size = 16)\n",
    "    plt.xticks(np.arange(0,23500,1500), size = 14)\n",
    "    plt.yticks(size = 14)\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_frequent_authors(df, col):\n",
    "    \n",
    "    plt.figure(figsize = (20,6))\n",
    "    sns.barplot(x = df.index,\n",
    "                y = col,\n",
    "                data = df)\n",
    "    plt.title(\"Most Common Posters\", size = 18)\n",
    "    plt.xlabel(\"Reddit User\", size = 16)\n",
    "    plt.ylabel(\"Number Of Posts\", size = 16)\n",
    "    plt.xticks(size = 13)\n",
    "    plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a list of text lengths\n",
    "\n",
    "length_list = [len(text) for text in model_data[\"text\"]]\n",
    "\n",
    "plot_text_length_dist(length_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the posts are relatively short (<2000 words), but there are a few that are extremely long (>20,000 words.)  We expected that most posts would be less than a few thousand words, which is true for the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Most Frequent Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_count = pd.DataFrame(model_data[\"author\"].value_counts().head(10))\n",
    "\n",
    "plot_most_frequent_authors(df  = author_count, \n",
    "                           col = \"author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not really know what to expect when we plotted this graph, because it is generally the case that a few users post most frequently and most barely post at all.  We would have like to look at the number of comments by each user in both subreddits as a measure of activity, but that is beyond the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subreddit Of Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels = [\"r/Cooking\", \"r/AskCulinary\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(model_data[\"source\"])\n",
    "plt.title(\"Post Origin\", size = 18)\n",
    "plt.xlabel(\"Source\", size = 16)\n",
    "plt.ylabel(\"Number Of Posts\", size = 16)\n",
    "plt.xticks(np.arange(0,2,1), \n",
    "           labels = tick_labels, \n",
    "           size = 14)\n",
    "plt.yticks(size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were a little surprised that there are more r/AskCulinary posts because we had roughly equal numbers of pulls from each subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Most Common Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start modeling, we need to know what the most frequent words are in each subreddit are because it might be harder for our model to predict with those words in the dataframe.\n",
    "\n",
    "We will subset the data frame into posts from r/Cooking and r/AskCulinary and use count vectorizer to determine the most frequent words.  We will also remove stop words from the outset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_frequent_words(dataframes, titles):\n",
    "    count = 0\n",
    "    fig   = plt.figure(figsize = (24,20))\n",
    "    for d, dataframe in enumerate(dataframes):\n",
    "        count += 1\n",
    "        ax    = fig.add_subplot(2, 2, count)\n",
    "        sns.barplot(x       = 0,\n",
    "                    y       = dataframe.index,\n",
    "                    data    = dataframe,\n",
    "                    palette = \"deep\")\n",
    "        plt.title(f\"Most Common Words From {titles[d]}\", size = 20)\n",
    "        plt.xlabel(\"Word\", size = 18)\n",
    "        plt.ylabel(\"Number Of Occurrences\", size = 18)\n",
    "        plt.xticks(size = 16)\n",
    "        plt.yticks(size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the count vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Masking the vectorizer with English stop words\n",
    "\n",
    "cvec_cooking     = CountVectorizer(stop_words = \"english\")\n",
    "cvec_askculinary = CountVectorizer(stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the dataframe\n",
    "\n",
    "cooking     = model_data[model_data[\"target\"] == 1]\n",
    "askculinary = model_data[model_data[\"target\"] == 0]\n",
    "\n",
    "# Fit-transforming the vectorizer\n",
    "\n",
    "vec_cooking     = cvec_cooking.fit_transform(cooking[\"text\"])\n",
    "vec_askculinary = cvec_askculinary.fit_transform(askculinary[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vectorized dfs to a new dataframe\n",
    "\n",
    "cooking_vectorized = pd.DataFrame(vec_cooking.toarray(), \n",
    "                                  columns = cvec_cooking.get_feature_names())\n",
    "\n",
    "askculinary_vectorized = pd.DataFrame(vec_askculinary.toarray(), \n",
    "                                      columns = cvec_askculinary.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing the vectorized dfs\n",
    "\n",
    "lemmatizer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the 15 most frequent words from each\n",
    "\n",
    "vectorized_cooking     = pd.DataFrame(cooking_vectorized.sum().sort_values(ascending = False).head(15))\n",
    "vectorized_askculinary = pd.DataFrame(askculinary_vectorized.sum().sort_values(ascending = False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the most common words\n",
    "\n",
    "plot_most_frequent_words(dataframes = [vectorized_cooking, vectorized_askculinary],\n",
    "                         titles     = [\"r/Cooking\", \"r/AskCulinary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of words that occur in both subreddits.  We decided that because of that, we should create a list of customized stop words.  Furthermore, we noticed that we have to lemmatize or stem the text columns because of there are multiple forms of words in the most frequent words such as 'make' & 'making' or 'recipe' and recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline in classification gives us an idea of how exactly the model is performing.  The baseline is simply the percentage of occurrences of our target in the data as a whole.  In this case it will be what percentage of posts are from r/Cooking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model has an accuracy of >41.44% we know that it is better than simply guessing the class of a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(model_data[\"source\"].value_counts(normalize = True)*100, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
