{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Scraping, Cleaning, & Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas             as pd\n",
    "import requests           as re\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "-----\n",
    "\n",
    "1. [Scraping The Reddit API](#Scraping-The-Reddit-API)\n",
    "    - [URLs & The User_Agent](#URLs-&-The-User-Agent)\n",
    "    - [Requests](#Requests)\n",
    "    - [Saving The Data](#Saving-The-Data)\n",
    "    - [Scraping](#Scraping)\n",
    "    - [Conversion To Dataframes](#Conversion-To-Dataframes)\n",
    "\n",
    "\n",
    "2. [Formatting](#Formatting)\n",
    "    - [Column Extraction](#Column-Extraction)\n",
    "    - [Creating New .csv Files](#Creating-New-.csv-Files)\n",
    "\n",
    "3. [Cleaning](#Cleaning)\n",
    "    - [Reading In The Data](#Reading-In-The-Data)\n",
    "    - [Removing Duplicates](#Removing-Duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping The Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit keeps raw data of all posts in a JSON format.  The documentation for its API can be found [here](https://www.reddit.com/dev/api/).\n",
    "\n",
    "Before we can start working with Reddit's API data, we have to set up a request for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs & The User_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be classifying posts from [r/Cooking](https://www.reddit.com/r/Cooking/) and [r/AskCulinary](https://www.reddit.com/r/AskCulinary), we need to have a URL and a `user_agent` for both subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For r/Cooking\n",
    "\n",
    "cooking_url = \"http://reddit.com/r/Cooking.json\"   \n",
    "user_agent  = {\"user-agent\": \"andrew_bergman\"}                        \n",
    "\n",
    "# For r/AskCulinary\n",
    "\n",
    "askculinary_url = \"http://reddit.com/r/AskCulinary.json\"\n",
    "user_agent      = {\"user-agent\": \"andrew_bergman\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we actually have the data, we can go ahead and set up the request.  The request is just a simple HTML request through the `requests` library.  We will also print out the status and hopefully get a status of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The r/Cooking request\n",
    "\n",
    "cooking_request  = re.get(url = cooking_url, headers = user_agent)\n",
    "\n",
    "# The r/ AskCulinary request\n",
    "\n",
    "askculinary_request  = re.get(url = askculinary_url, headers = user_agent)\n",
    "\n",
    "# Print status codes\n",
    "\n",
    "print(f\"The r/Cooking status code is    : {cooking_request.status_code}\")\n",
    "print(f\"The r/AskCulinary status code is: {askculinary_request.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two working requests, we can go ahead and save the data as a variable.  To do that, we will convert the data to a JSON object and save that as the variable.\n",
    "\n",
    "\n",
    "Because we already have some older posts, we will be adding the new pulls to our old ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the new r/Cooking data\n",
    "\n",
    "new_cooking_data     = cooking_request.json()\n",
    "\n",
    "# Saving the new r/AskCulinary data\n",
    "\n",
    "new_askculinary_data = askculinary_request.json()\n",
    "\n",
    "\n",
    "# Checking to make sure I got 25 posts from my first pull\n",
    "\n",
    "print(f'The initial r/Cooking request returned    : {len(new_cooking_data[\"data\"][\"children\"])}')\n",
    "print(f'The initial r/AskCulinary request returned: {len(new_askculinary_data[\"data\"][\"children\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the `id`s from both pulls\n",
    "\n",
    "print(f'The r/Cooking ID is    : {new_cooking_data[\"data\"][\"after\"]}')\n",
    "print(f'The r/AskCulinary ID is: {new_askculinary_data[\"data\"][\"after\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reddit API allows for 1,000 posts to be scraped per subreddit per day.  In total we will have roughly 2,000 posts in addition to the older scraped data.\n",
    "\n",
    "To make the scraping easier, we made use of a `for` loop to scrape the API 40 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping r/Cooking\n",
    "\n",
    "# Creating an empty list to save the scrapes to\n",
    "new_cooking_posts = []\n",
    "\n",
    "# Setting it to `None` for use in the loop\n",
    "after         = None\n",
    "\n",
    "for pull in range(40):\n",
    "    \n",
    "    # Tells us the post being scraped in case of errors\n",
    "    print(f\"Pull Attempt {pull + 1}\")\n",
    "    \n",
    "    if after == None:\n",
    "        \n",
    "        # Sets up the initial loop\n",
    "        new_url = cooking_url\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Allows for the creation of the next pull\n",
    "        new_url = cooking_url + \"?after=\" + after\n",
    "        \n",
    "    # Resetting the request    \n",
    "    request = re.get(url = new_url, headers = user_agent)\n",
    "    \n",
    "    # Only works if the status is good\n",
    "    if request.status_code == 200:\n",
    "        # creates a new dictionary & then appends it to the empty list\n",
    "        new_cooking_data = request.json()\n",
    "        new_cooking_posts.extend(new_cooking_data[\"data\"][\"children\"])\n",
    "        \n",
    "        # Sets a new after value\n",
    "        after = new_cooking_data[\"data\"][\"after\"]\n",
    "        \n",
    "    else:\n",
    "        print(f\"An Error Has Occurred.  Error Code {request.status_code}\")\n",
    "        break\n",
    "        \n",
    "    # Setting a sleep time prevents me from being interpretted as a bot        \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For r/AskCulinary\n",
    "\n",
    "\n",
    "new_askculinary_posts = []\n",
    "after             = None\n",
    "\n",
    "for pull in range(40):\n",
    "    print(f\"Pull Attempt {pull + 1}\")\n",
    "    if after == None:    \n",
    "        new_url = askculinary_url\n",
    "    else:\n",
    "        new_url = askculinary_url + \"?after=\" + after\n",
    "    new_askculinary_request = re.get(url = new_url, headers = user_agent)\n",
    "    if new_askculinary_request.status_code == 200:\n",
    "        new_askculinary_data = new_askculinary_request.json()\n",
    "        new_askculinary_posts.extend(new_askculinary_data[\"data\"][\"children\"])\n",
    "        after = new_askculinary_data[\"data\"][\"after\"]\n",
    "    else:\n",
    "        print(f\"An Error Has Occurred.  Error Code {askculinary_request.status_code}\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion To Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 1,000 posts from each subreddit, we chose to save them as Pandas dataframes because it is easier to manipulate them in that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For r/Cooking posts\n",
    "\n",
    "new_cooking_data     = pd.DataFrame(new_askculinary_posts)\n",
    "\n",
    "# For r/AskCulinary posts\n",
    "\n",
    "new_askculinary_data = pd.DataFrame(new_askculinary_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the scraped data is now in the form of a dataframe, we cannot work with it yet because the entirety of the posts data is in the form of a dictionary in each cell.  To be able to work with the data, we will have to extract certain key-value pairs from the data and have those be features in the modified dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dictionary for each post has a lot of key-value pairs, but we only need four: `id`, `author`, `title`, and `selftext`.\n",
    "\n",
    "While most subreddits are image or video based, we are lucky in that r/Cooking and r/AskCulinary are primarily text based communities: in addition to the title we have a body of text written by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the r/Cooking data\n",
    "\n",
    "# Using list comprehension to create new columns\n",
    "\n",
    "cooking_id    = [new_cooking_data['data'][post]['id'] for post in range(len(new_cooking_data['data']))]\n",
    "cooking_auth  = [new_cooking_data['data'][post]['author'] for post in range(len(new_cooking_data['data']))]\n",
    "cooking_title = [new_cooking_data['data'][post]['title'] for post in range(len(new_cooking_data['data']))]\n",
    "cooking_self  = [new_cooking_data['data'][post]['selftext'] for post in range(len(new_cooking_data['data']))]\n",
    "\n",
    "# Creating new columns and setting them equal to the list comprehension results\n",
    "\n",
    "new_cooking_data[\"id\"]       = cooking_id\n",
    "new_cooking_data[\"title\"]    = cooking_title\n",
    "new_cooking_data[\"selftext\"] = cooking_self\n",
    "new_cooking_data[\"author\"]   = cooking_auth\n",
    "new_cooking_data[\"source\"]   = \"cooking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the r/AskCulinary data:\n",
    "\n",
    "# Using list comprehension to create new columns\n",
    "\n",
    "askcul_id    = [new_askculinary_data['data'][post]['id'] for post in range(len(new_askculinary_data['data']))]\n",
    "askcul_auth  = [new_askculinary_data['data'][post]['author'] for post in range(len(new_askculinary_data['data']))]\n",
    "askcul_title = [new_askculinary_data['data'][post]['title'] for post in range(len(new_askculinary_data['data']))]\n",
    "askcul_self  = [new_askculinary_data['data'][post]['selftext'] for post in range(len(new_askculinary_data['data']))]\n",
    "\n",
    "# Creating new columns and setting them equal to the list comprehension results\n",
    "\n",
    "new_askculinary_data[\"id\"]            = askcul_id\n",
    "new_askculinary_data[\"title\"]         = askcul_title\n",
    "new_askculinary_data[\"selftext\"]      = askcul_self\n",
    "new_askculinary_data[\"author\"]        = askcul_auth\n",
    "new_askculinary_data[\"source\"]        = \"askculinary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to make sure that the two are dataframs\n",
    "\n",
    "print(f\"The r/Cooking data is a    : {type(new_cooking_data)}\")\n",
    "print(f\"The r/AskCulinary data is a: {type(new_askculinary_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the format of the r/Cooking data\n",
    "\n",
    "new_cooking_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the format of the r/Cooking data\n",
    "\n",
    "new_askculinary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New .csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a r/Cooking .csv file\n",
    "\n",
    "new_cooking_data.to_csv(\"../Data/new_cooking_df.csv\")\n",
    "\n",
    "# Creating a r/AskCulinary .csv file\n",
    "\n",
    "new_askculinary_data.to_csv(\"../Data/new_askculinary_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Reddit data in the format we want it in, we have to start cleaning it.  The process involves removing unnecessary columns, removing punctuation and non alphanumeric characters, and removing null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading In The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_cooking_data = pd.read_csv(\"../Data/cooking_df.csv\")\n",
    "new_cooking_data = pd.read_csv(\"\")\n",
    "\n",
    "old_askculinary_data = pd.read_csv(\"../Data/askculinary_df.csv\")\n",
    "new_askculinary_data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
